# Mini AI Analyst

A comprehensive data analysis and machine learning platform that provides automated data profiling, model training, predictions, and AI-powered insights using Google's Gemini LLM.
Deployed at : https://orthoaiassgnment.vercel.app/

## üöÄ Features

- **Data Upload & Processing**: Upload CSV files and get instant data profiling
- **Automated Data Analysis**: Comprehensive statistical analysis, outlier detection, and correlation analysis
- **Machine Learning Pipeline**: Train classification and regression models with multiple algorithms
- **Model Predictions**: Make predictions on new data with confidence scores
- **Natural Language Summaries**: Get detailed insights and recommendations in plain English
- **AI-Powered Analysis**: Advanced insights generated by Google's Gemini 2.0 Flash LLM
- **Export Capabilities**: Export comprehensive reports and analysis results
- **User Authentication**: Secure user management with Supabase
- **Background Task Processing**: Asynchronous processing with Celery
- **Real-time Monitoring**: Task monitoring with Flower
- **Production-Ready**: Nginx reverse proxy with SSL support

## üß† AI Analysis Features

The platform now includes an advanced AI Analysis section powered by Google's Gemini LLM that provides:

- **Comprehensive AI Analysis**: Natural language analysis of your dataset and model performance
- **Enhanced Insights**: Deeper insights that go beyond basic statistics
- **Business Recommendations**: Actionable business recommendations based on data insights
- **Technical Recommendations**: Technical recommendations for improving models and data quality
- **Risk Assessment**: Identification of potential risks and concerns
- **Opportunities**: Business opportunities and areas for further investigation

## üõ†Ô∏è Technology Stack

### Backend
- **FastAPI**: Modern Python web framework
- **Pandas & NumPy**: Data manipulation and analysis
- **Scikit-learn**: Machine learning algorithms
- **XGBoost**: Gradient boosting framework
- **SHAP**: Model interpretability
- **Google Generative AI**: Gemini LLM integration
- **Supabase**: Database and authentication
- **Celery**: Background task processing
- **Redis**: Caching and message broker
- **PostgreSQL**: Primary database

### Frontend
- **React**: User interface framework
- **TypeScript**: Type-safe JavaScript
- **Tailwind CSS**: Utility-first CSS framework
- **Lucide React**: Beautiful icons
- **React Hot Toast**: Toast notifications

### Infrastructure
- **Docker**: Multi-stage containerization with optimized builds
- **Nginx**: Reverse proxy and load balancing
- **Flower**: Celery monitoring
- **Celery Beat**: Scheduled tasks

## üì¶ Installation

### Prerequisites
- Python 3.10+
- Node.js 16+
- Docker & Docker Compose

### Environment Configuration
Create a `.env` file in the root directory:
```env
# Required for AI Analysis
GEMINI_LLM_API_KEY=your_gemini_api_key_here

# Supabase Configuration (Required)
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key

# Database Configuration (Optional - defaults to local PostgreSQL)
DATABASE_URL=postgresql://user:password@localhost/ai_analyst
POSTGRES_DB=ai_analyst
POSTGRES_USER=user
POSTGRES_PASSWORD=password

# Redis Configuration (Optional - for background tasks)
REDIS_URL=redis://localhost:6379/0

# JWT Configuration
SECRET_KEY=your-secret-key-change-in-production

# S3 Storage (Optional)
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_REGION=us-east-1
S3_BUCKET_NAME=your_bucket_name

# Frontend Configuration
REACT_APP_API_URL=http://localhost:8000/api/v1
```

## üöÄ Running the Application

### Development Mode
```bash
# Backend
cd backend
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload --port 8000

# Frontend
cd frontend
npm install
npm start
```

### Docker Development
```bash
# Start all services
docker-compose up --build

# Start without optional services
docker-compose up --build backend frontend postgres redis celery

# Start with production profile (includes nginx)
docker-compose --profile production up --build
```

### Docker Production
```bash
# Start with production configuration
docker-compose --profile production up --build -d

# View logs
docker-compose logs -f

# Scale services
docker-compose up --scale celery=3 --scale backend=2
```

## üê≥ Docker Services

The application includes the following Docker services:

### Core Services
- **backend**: FastAPI application (port 8000)
- **frontend**: React application (port 3000)
- **postgres**: PostgreSQL database (port 5432)
- **redis**: Redis cache and message broker (port 6379)
- **celery**: Background task worker

### Optional Services
- **celery-beat**: Scheduled task scheduler
- **flower**: Celery monitoring dashboard (port 5555)
- **nginx**: Reverse proxy (ports 80/443, production profile)

### Service URLs
- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs
- **Flower Monitoring**: http://localhost:5555 (when enabled)
- **Nginx Proxy**: http://localhost (production profile)

### Frontend Docker Targets
The frontend Dockerfile supports multiple build targets for different environments:

- **Production** (`FRONTEND_TARGET=production`): Nginx-based serving with security headers and compression
- **Development** (`FRONTEND_TARGET=development`): Hot-reload development server with volume mounting
- **Testing** (`FRONTEND_TARGET=testing`): Isolated testing environment
- **Build-only** (`FRONTEND_TARGET=build-only`): CI/CD optimized build stage

```bash
# Production frontend
FRONTEND_TARGET=production docker-compose up frontend

# Development frontend with hot-reload
FRONTEND_TARGET=development FRONTEND_VOLUMES="./frontend:/app,/app/node_modules" docker-compose up frontend

# Testing frontend
FRONTEND_TARGET=testing docker-compose up frontend
```

## üìä API Endpoints

### 1. Upload Data
```http
POST /api/v1/upload
```

### 2. Data Profiling
```http
GET /api/v1/profile/{session_id}
```

### 3. Model Training
```http
POST /api/v1/train/{session_id}
```

### 4. Predictions
```http
POST /api/v1/predict/{model_id}
```

### 5. Summary
```http
GET /api/v1/summary/{session_id}
```

### 6. AI Analysis
```http
GET /api/v1/ai-analysis/{session_id}
```

### 7. Export
```http
GET /api/v1/export/report/{session_id}
```

### 8. Authentication
```http
POST /api/v1/auth/register
POST /api/v1/auth/login
GET /api/v1/auth/profile
```

### 9. Dashboard
```http
GET /api/v1/dashboard/stats
GET /api/v1/dashboard/sessions
GET /api/v1/dashboard/models
GET /api/v1/dashboard/predictions
```

### 10. Background Tasks
```http
POST /api/v1/background-tasks/train/{session_id}
POST /api/v1/background-tasks/ai-analysis/{session_id}
GET /api/v1/background-tasks/status/{task_id}
```

### 11. Monitoring
```http
GET /api/v1/monitoring/redis/stats
GET /api/v1/monitoring/redis/health
```

## üß™ Testing

### Backend Tests
```bash
cd backend
pytest -v
```

### Frontend Tests
```bash
cd frontend
npm test
```

### Docker Tests
```bash
# Run tests in Docker
docker-compose run backend pytest -v
docker-compose run frontend npm test
```

## üìä Sample Use Case

1. **Upload Dataset**: User uploads a customer churn dataset
2. **Data Profiling**: System automatically analyzes the data and provides insights
3. **Model Training**: User selects "churn" as target and trains a classification model
4. **Predictions**: User can make predictions on new customer data
5. **Summary**: System provides natural language summary of findings
6. **AI Analysis**: Advanced AI-powered insights using Gemini LLM

## üîß Configuration

### Environment Variables
- `MAX_FILE_SIZE`: Maximum file size in bytes (default: 50MB)
- `MODEL_STORAGE_PATH`: Path to store trained models
- `UPLOAD_STORAGE_PATH`: Path to store uploaded files
- `CORS_ORIGINS`: Allowed CORS origins for frontend
- `GEMINI_LLM_API_KEY`: Google Gemini API key for AI analysis
- `SUPABASE_URL`: Supabase project URL
- `SUPABASE_ANON_KEY`: Supabase anonymous key
- `SUPABASE_SERVICE_ROLE_KEY`: Supabase service role key
- `DATABASE_URL`: PostgreSQL database connection string
- `REDIS_URL`: Redis connection string for background tasks
- `SECRET_KEY`: JWT secret key for authentication

### Docker Configuration
- **Resource Limits**: Each service has defined CPU and memory limits
- **Health Checks**: All services include health check endpoints
- **Networking**: Services communicate via internal Docker network
- **Volumes**: Persistent storage for database, cache, and file uploads

## üöÄ Deployment

### Production Docker
```bash
# Start with production profile
docker-compose --profile production up --build -d

# Monitor services
docker-compose ps
docker-compose logs -f
```

### Cloud Deployment
The application is designed to be easily deployable to:
- **AWS ECS/Fargate**: Use the provided Dockerfiles
- **Google Cloud Run**: Containerized deployment
- **Azure Container Instances**: Direct container deployment
- **Heroku**: Container deployment
- **Kubernetes**: Use the Docker images with K8s manifests

### Scaling
```bash
# Scale backend services
docker-compose up --scale backend=3 --scale celery=5

# Scale with resource limits
docker-compose up --scale backend=2 --scale celery=3
```

## üìù Assumptions and Limitations

### Assumptions
- CSV files are well-formatted with headers
- Target columns are clearly identifiable
- Users have basic understanding of ML concepts
- File uploads are from trusted sources

### Limitations
- Maximum file size: 50MB
- Supported formats: CSV only
- Model types: Classification and Regression
- No real-time streaming for large datasets
- Basic authentication (session-based)
- AI Analysis requires valid Gemini API key

### Performance Considerations
- Large datasets may require increased memory limits
- ML training can be CPU-intensive
- Redis memory usage should be monitored
- Database connections should be pooled for high traffic

## üéØ Optional Features Implemented

- ‚úÖ Background job processing with Celery
- ‚úÖ PostgreSQL database for metadata storage
- ‚úÖ S3-compatible storage for files
- ‚úÖ Model versioning and management
- ‚úÖ Comprehensive data validation
- ‚úÖ Automated feature engineering
- ‚úÖ Model interpretability with SHAP
- ‚úÖ Export functionality (JSON, CSV, ZIP)
- ‚úÖ AI-powered analysis with Gemini LLM
- ‚úÖ Real-time progress tracking
- ‚úÖ Error handling and recovery
- ‚úÖ Responsive design
- ‚úÖ Multi-stage Docker containerization
- ‚úÖ Nginx reverse proxy
- ‚úÖ Celery monitoring with Flower
- ‚úÖ Resource management and limits
- ‚úÖ Health checks and monitoring
- ‚úÖ Production-ready configuration
- ‚úÖ Security enhancements (non-root user)
- ‚úÖ Optimized image builds

## üîç Monitoring and Debugging

### Health Checks
- **Backend**: `GET /health` - Checks API and Redis connectivity
- **Frontend**: Web server availability check
- **PostgreSQL**: Database connectivity check
- **Redis**: Cache connectivity check
- **Celery**: Worker availability check

### Logs
```bash
# View all logs
docker-compose logs -f

# View specific service logs
docker-compose logs -f backend
docker-compose logs -f celery

# View logs with timestamps
docker-compose logs -f --timestamps
```

### Monitoring
- **Flower**: http://localhost:5555 - Celery task monitoring
- **API Docs**: http://localhost:8000/docs - Interactive API documentation
- **Health Endpoints**: Various health check endpoints for each service

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass
6. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üÜò Support

For support and questions:
- Check the API documentation at `/docs`    
- Review the health check endpoints
- Check the logs for error details
- Monitor the Flower dashboard for task status

##TIME
10-11 hours 

